{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgW4xmGSIzSsoo+T9w4F/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabuGravin41/Bank_account.-py/blob/main/Linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<h1 style=\"text-align:center\"><strong> Linear Regression</strong></h1>\n",
        "<p style=\"text-align:center\">Authored by <b>Dalton Omondi</b></p>\n",
        "<p style=\"text-align:center\">Electrical and Electronics Engineering, Kenyatta University</b></p>\n",
        "<p style=\"text-align:center\">June 2024</p>"
      ],
      "metadata": {
        "id": "1SqTPkEDG1Qa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "c3d218ae-7414-45d9-97b1-cc649dd1ed55"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1 style=\"text-align:center\"><strong> Linear Regression</strong></h1>\n",
              "<p style=\"text-align:center\">Authored by <b>Dalton Omondi</b></p>\n",
              "<p style=\"text-align:center\">Electrical and Electronics Engineering, Kenyatta University</b></p>\n",
              "<p style=\"text-align:center\">June 2024</p>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Latex\n",
        "Latex(r'\\usepackage{amsmath}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "dEiFaZ52--Bo",
        "outputId": "862c64f9-bdb8-4658-99a8-694b3cc846e4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ],
            "text/latex": "\\usepackage{amsmath}"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Linear Regression** is a type of supervised learning.\n",
        "\n",
        "It makes use of labels in a dataset to map data points to the most optimized linear fuction that can be used for prediction of new data points.\n",
        "\n",
        "## PS: Supervised Learning SL\n",
        "In SL, ML algorithms learn from labelled data.\n",
        "\n",
        "Labelled data --> respective target value is already known\n",
        "\n",
        "## Types of SL\n",
        "\n",
        "--- Classification\n",
        "\n",
        "--- Regression\n",
        "\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "Computes linear relationship between ***'dependent variables'*** and oen or more ***'indipendent variables'*** .\n",
        "\n",
        "\n",
        "--- **UNIVARIATE** -- one indipendent feature\n",
        "\n",
        "\n",
        "--- **MULTIVARIATE** -- many indipendent features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lXa2E3diYunY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TERMINOLOGY"
      ],
      "metadata": {
        "id": "NkN6ZTGHyv2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Learning\n",
        "\n",
        "\n",
        "\n",
        "--- A function $h$ (hypothesis) is used to map input $X$ into the predicted output $\\hat{Y}$.\n",
        "$$\n",
        "\\boldsymbol{h : X \\to \\hat{Y}}\n",
        "$$\n",
        "$Y \\to$  actual output\n",
        "\n",
        "$\\hat{Y} \\to $ predicted output\n",
        "\n",
        "$h_\\theta(X) \\to $ hypothesis on $X$ parametrized by $\\theta$\n",
        "\n",
        "i.e **$h_{ears, fur, whiskers}$ {images} $\\to$ {cat}**\n"
      ],
      "metadata": {
        "id": "AmUh4-D9y45W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a data set $\\{y_i, \\, x_{i1}, \\ldots, x_{ip}\\}_{i=1}^n$  of  $n$ statistical units, a linear regression model assumes that the relationship between the dependent variable $y$ and the vector of regressors $\\mathbf{x}$ is linear. This relationship is modeled through a disturbance term or **error variable $\\varepsilon$** — an unobserved random variable that adds ***noise*** to the linear relationship between the dependent variable and regressors. Thus the model takes the form:\n",
        "\n",
        "$$\n",
        "Y_i = \\theta_0 + \\theta_1 x_{i1} + \\cdots + \\theta_p x_{ip} + \\varepsilon_i = \\mathbf{x}_i^{\\mathsf{T}}\\boldsymbol{\\theta} + \\varepsilon_i, \\qquad i=1,\\ldots,n,\n",
        "$$\n",
        "\n",
        "where $^\\mathsf{T}$ denotes the transpose, so that $\\mathbf{x}_i^{\\mathsf{T}}\\boldsymbol{\\theta}$ is the inner product between vectors $\\mathbf{x}_i$ and $\\boldsymbol{\\theta}$.\n",
        "\n",
        "Often these $n$ equations are stacked together and written in matrix notation as:\n",
        "\n",
        "$$\n",
        "Y =  X^{\\mathsf{T}}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}\n",
        "  = \\sum_{i=0}^{d} \\theta_i \\mathbf{x}_i + \\boldsymbol{\\varepsilon}\n",
        "  = h_\\theta(X) + \\boldsymbol{\\varepsilon}\n",
        "$$\n",
        "where:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_0 = 1 ;\\\\ \\\\\n",
        "\\mathbf{y} = \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix}, \\quad\n",
        "X = \\begin{bmatrix}\n",
        "\\mathbf{x}_1^{\\mathsf{T}} \\\\\n",
        "\\mathbf{x}_2^{\\mathsf{T}} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{x}_n^{\\mathsf{T}}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1 & x_{11} & \\cdots & x_{1p} \\\\\n",
        "1 & x_{21} & \\cdots & x_{2p} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{n1} & \\cdots & x_{np}\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\theta} = \\begin{bmatrix}\n",
        "\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_p\n",
        "\\end{bmatrix}, \\quad\n",
        "\\boldsymbol{\\varepsilon} = \\begin{bmatrix}\n",
        "\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\n",
        "\\end{bmatrix}.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "oaW_ma87DrPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assumptions of Linear Regression\n",
        "\n",
        "Taking predictors/indipendent features $x_{i}$ *( we will call them features in future)* and output variables $Y_{i}$ and errors $\\varepsilon_i$.\n",
        "\n",
        "\n",
        " ***errors = Predicted value - Actual value***\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.  **Linearity**\n",
        "\n",
        "\n",
        "*--- relationship between features $x_{i}$ and outputs $Y_{i}$ is assumed to be linear.*\n",
        "\n",
        "\n",
        "Note that this assumption is a lot less restrictive than it may at first seem. In fact, Linearity is a restriction on just the parameters $\\theta_j$. The features $X_i$ can be arbitrarily transformed. An example is in **polynomial regression** which makes use of linear regression to fit features to an arbitrary polynomial up to a given degree. This much flexibility leads ot too muchpower which tends to lead to overfitting. Therefore, some reguarisation techniques are needed i.e Bayesian Regression( Ridge reg, Lasso reg e.t.c)\n",
        "2.   **Statistical Indipendence**\n",
        "\n",
        "\n",
        "*--- observations and errors are assumed to be statistically indipendent*\n",
        "\n",
        "\n",
        "Some methods such as generalized least squares are capable of handling correlated errors, although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general way of handling this issue.\n",
        "\n",
        "3.   **Homoscedasticity**\n",
        "\n",
        "*--- the number of variables does not affest the variance  $\\sigma^2$ i.e  $\\sigma^2$  is constant*\n",
        "\n",
        "\n",
        "Thus the variability of the responses for given fixed values of the predictors is the same regardless of how large or small the responses are. This is often not the case, as a variable whose mean is large will typically have a greater variance than one whose mean is small. For example, a person whose income is predicted to be $ \\$100,000$ may easily have an actual income of $80,000 or $120,000—i.e., a standard deviation of around $\\$20,000$ —while another person with a predicted income of $\\$10,000$ is unlikely to have the same $\\$20,000$ standard deviation, since that would imply their actual income could vary anywhere between $ -\\$10,000$ and $\\$30,000.$ (In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.)\n",
        "4.   **Normality**\n",
        "\n",
        "\n",
        "*---residuals/ errors are normally distributed.*\n",
        "\n",
        "---*The statement* ***Indipendent and Identically Distributed IID***  *is usually used*.\n",
        "\n",
        "\n",
        "The general assumption here is that the data distribution is a member of the ***exponential distribution*** family and in particular is distributed gaussian.\n",
        "\n",
        "We write:\n",
        "\n",
        "$$\n",
        "(Y|X; \\theta) \\sim \\mathcal{N}(0, σ2\n",
        ")\n",
        "$$\n",
        "\n",
        "5.   **No multicollinearity**\n",
        "\n",
        "\n",
        "*---there exists little to no correlation between indipendent variables*\n",
        "\n",
        "\n",
        "Near violations of this assumption, where predictors are highly but not perfectly correlated, can reduce the precision of parameter estimates (see Variance inflation factor). In the case of perfect multicollinearity, the parameter vector $\\theta$ will be non-identifiable—it has no unique solution. In such a case, only some of the parameters can be identified (i.e., their values can only be estimated within some linear subspace of the full parameter space $\\mathcal{R^p}$.)\n",
        "\n",
        "**Variance inflation factor(VIF)** is the ratio (quotient) of the variance of a parameter estimate *when fitting a full model that includes other parameters* to the variance of the parameter estimate *if the model is fit with only the parameter on its own*.\n",
        "6.   **Assumption of Zero Mean of Residuals**\n",
        "\n",
        "*---the mean of the residuals is zero or close to zero.*\n",
        "\n",
        "This assumption is fundamental for the validity of any conclusions drawn from the least squares estimates of the parameters.\n",
        "If the mean of these residuals is not zero, it implies that the model consistently overestimates or underestimates the observed values, indicating a potential bias in the model estimation. Ensuring that the mean of the residuals is zero allows the model to be considered unbiased in terms of its error, which is crucial for the accurate interpretation of the regression coefficients.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y4UhKmi3Gbjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LEAST MEAN SQUARES**\n",
        "### **:** *How do we choose params* $\\theta$\n",
        "\n",
        "\n",
        "We use **Least squares fit**,  also known as the ***Widrow-Hoff*** $ \\\\ $ learning rule.\n",
        "\n",
        "i.e choose $\\theta$ to minimize the square of the errors on the estimated function from the data.\n",
        "$$$$\n",
        "$$\n",
        "argmin_{\\boldsymbol{\\theta}} \\left(\\boldsymbol{\\epsilon}^2\\right) = (\\boldsymbol{h}_{\\boldsymbol{\\theta}}(X) - Y)^2\n",
        "$$\n",
        "$$$$\n",
        "But we do this for each training example $(X^\\boldsymbol{(i)}, Y^\\boldsymbol{(i)})$\n",
        "\n",
        "$\\ \\ \\boldsymbol{\\therefore}$ Summing over all $(X^\\boldsymbol{(i)}, Y^\\boldsymbol{(i)}):$\n",
        "$$\n",
        "argmin_{\\boldsymbol{\\theta}} \\ \\  \\frac{1}{2}\\sum_{i=1}^{m} \\ \\ (\\boldsymbol{h}_{\\boldsymbol{\\theta}}(X^\\boldsymbol{(i)}) - Y^\\boldsymbol{(i)})^2\n",
        "\\ \\ \\ \\ \\ \\ \\ \\  i= 1,2,...,m\n",
        "$$\n",
        "\n",
        "We define the expression above as the **cost function** $J(\\theta)$\n",
        "\n",
        "Note: $$\n",
        "Cost function:\n",
        "J(\\boldsymbol{\\theta}) = argmin_{\\boldsymbol{\\theta}} \\ \\  \\frac{1}{2}\\sum_{i=1}^{m} \\ \\ (\\boldsymbol{h}_{\\boldsymbol{\\theta}}(X^\\boldsymbol{(i)}) - Y^\\boldsymbol{(i)})^2\n",
        "\\ \\ \\ \\ \\ \\ \\ \\  i= 1,2,...,m\n",
        "$$$$$$\n",
        "$$\n",
        "Loss function:\n",
        "L(\\boldsymbol{\\theta}) = argmin_{\\boldsymbol{\\theta}} \\ \\  \\frac{1}{2}\\ \\ (\\boldsymbol{h}_{\\boldsymbol{\\theta}}(X^\\boldsymbol{(i)}) - Y^\\boldsymbol{(i)})^2\n",
        "\\ \\ \\ \\ \\ \\ \\ \\  i= 1,2,...,m \\\\ Only\\ \\ one\\ \\ training\\ \\ example\n",
        "$$\n",
        "$$$$\n",
        "\n",
        "### ***Gradient descent Algorithm :***\n",
        "\n",
        "--- Utilizing the observation that $L(\\boldsymbol\\theta)$ and $J(\\boldsymbol\\theta)$, we will update $\\boldsymbol{\\theta_j}$ so as to descend down the quadratic.\n",
        "\n",
        "**Update Function :**\n",
        "$$\n",
        "\\boldsymbol{\\theta_j} := \\theta_j - \\alpha \\ \\nabla_{\\theta_j} J(\\theta)\n",
        "$$\n",
        "\n",
        "This update is simultaneously performed for all values of $j = 0, \\ldots, d$.\n",
        "\n",
        "Here, $\\boldsymbol\\alpha$ is called the **learning rate**. *In industry, it's common practice to first set it to 0.01 and see how the model performs with that.*\n",
        "\n",
        "This is a very natural algorithm that repeatedly takes a step in the direction of *steepest decrease* of $J$.\n",
        "$$$$\n",
        "\n",
        "Now to compute $\\nabla_{\\theta_j} J(\\boldsymbol\\theta)$:\n",
        "$$\n",
        "\\nabla_{\\theta_j} J(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\theta_j}J(\\theta) \\\\\n",
        "\\implies \\frac{\\partial}{\\partial \\theta_j}J(\\theta) \\left[\\frac{1}{2}\\sum_{i=1}^{m}(\\boldsymbol{h}_{\\boldsymbol{\\theta}}(X^{(i)}) - Y^{(i)})^2\\right] \\\\\n",
        "\\implies \\sum_{i=1}^{m}(\\boldsymbol{h}_{\\boldsymbol{\\theta}}(X^{(i)}) - Y^{(i)})\\frac{\\partial}{\\partial \\theta_j}\\ (\\boldsymbol{h}_{\\boldsymbol{\\theta}}(X^{(i)}) - Y^{(i)}) \\\\\n",
        "\\implies \\sum_{i=1}^{m}(\\boldsymbol{h}_{\\boldsymbol{\\theta}}(X^{(i)}) - Y^{(i)}) \\frac{\\partial}{\\partial \\theta_j}\\ \\left(\\sum_{i=1}^{m} \\theta_j \\mathbf{x_j^{(i)}} - Y\\right) \\\\\n",
        "\\implies \\sum_{i=1}^{m}(\\boldsymbol{h}_{\\boldsymbol{\\theta}}(x^{(i)}) - Y^{(i)}) \\mathbf{x_j^{(i)}}\\ \\ \\ \\ \\  \\ \\ \\ \\\n",
        "$$\n",
        "$$$$\n",
        "For one training example, we have that:\n",
        "$$\n",
        "\\theta_j := \\theta_j - \\alpha (\\boldsymbol{h}_{\\boldsymbol{\\theta}}(x^{(i)}) - Y^{(i)}) x_j^{(i)} \\ \\to \\ \\ \\ Stochastic\\ gradient\\ descent\n",
        "$$\n",
        "For all the training examples:\n",
        "$$\n",
        "\\theta_j := \\theta_j - \\alpha \\sum_{i=1}^{m}(\\boldsymbol{h}_{\\boldsymbol{\\theta}}(x^{(i)}) - Y^{(i)}) x_j^{(i)} \\ \\to \\ \\ Batch\\ gradient\\ descent\n",
        "$$\n",
        "This algorithm is repeated for all $\\theta_j$ and each training example $\\mathbf{(X_j^m, Y_j^m)}$ ; for m is the number of training examples -- until convergence.\n",
        "\n",
        "$$$$\n",
        "### **Batch Gradient Descent**\n",
        "\n",
        "We say 'batch' when we do the update on $\\theta_j$ using the entire training set, per iteration.\n",
        "\n",
        "\n",
        "**Algorithm :**\n",
        "\n",
        "1.  Pick some $\\theta_j$ ( conviniently closer to the optimum).  \n",
        "\n",
        "2.  Apply the batch gradient descent update function:\n",
        "$\n",
        "\\theta_j := \\theta_j - \\alpha \\sum_{i=1}^{m}(\\boldsymbol{h}_{\\boldsymbol{\\theta}}(x^{(i)}) - Y^{(i)}) x_j^{(i)} \\ \\to \\ \\ Batch\\ gradient\\ descent\n",
        "$\n",
        "\n",
        "3.  Repeat until convergence.\n",
        "\n",
        "\n",
        "#### **Disadvantages of Batch gradient descent**\n",
        "\n",
        "--- It is **very slow** for large datasets\n",
        "\n",
        "    *if you have **terabytes** of data, its better to use stochatic gradient descent*\n",
        "\n",
        "\n",
        "  $$$$\n",
        "### **Stochastic Gradient Descent**\n",
        "\n",
        "Update $\\theta_j$ with only one training example at a time.\n",
        "\n",
        "**Algorithm :**\n",
        "\n",
        "  Repeat {\n",
        "    \n",
        " $\\ \\ \\ $For i=1 to m {\n",
        "\n",
        " $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\  $ $\n",
        "  \\theta_j := \\theta_j - \\alpha (\\boldsymbol{h}_{\\boldsymbol{\\theta}}(x^{(i)}) - Y^{(i)}) x_j^{(i)}\n",
        "  $\n",
        "\n",
        "  $\\ \\ \\ \\ \\ \\ \\ \\  $}\n",
        "\n",
        "  $\\ \\ \\ $}\n",
        "\n"
      ],
      "metadata": {
        "id": "b-I3medE8RRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://media.licdn.com/dms/image/C4D12AQHXDuH9C-vZqQ/article-inline_image-shrink_400_744/0/1637174337571?e=1721260800&v=beta&t=-cmwtBvvHr7MzTSsHpv6bRUC9qtsPM3dGbyGd3ZmRLQ')\n"
      ],
      "metadata": {
        "id": "kO0yu3lOZHN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "f0f12f01-1fd6-487b-9e8e-2261487c78c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://media.licdn.com/dms/image/C4D12AQHXDuH9C-vZqQ/article-inline_image-shrink_400_744/0/1637174337571?e=1721260800&v=beta&t=-cmwtBvvHr7MzTSsHpv6bRUC9qtsPM3dGbyGd3ZmRLQ\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image above uses contour lines to illustrate the difference between Batch and Stochastic gradient descents.\n",
        "\n",
        "The shortest path down a valley with contours is a straight line orthogonal to the contours. This is the path taken by ***Batch gradient descent***.\n",
        "\n",
        "***Stochastic gradient descent*** is rather chaotic but offers a faster way of reaching convergence for larger datasets since we do't have to sum over the entire dataset at every iteration."
      ],
      "metadata": {
        "id": "tJUjbM903Lru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    *Techique for stoachastic gradient descent*\n",
        "    ---As you approach the optimum,  decrease the learning rate $\\alpha$ to avoid overshooting past the optimum.\n",
        "\n",
        "$$$$\n",
        "#### **When to stop stochastic gradient descent**\n",
        "\n",
        " ***(How do you tell you have reached optimum)***\n",
        "\n",
        "\n",
        "\n",
        "*   Plot the loss function $L(\\theta)$ against time and observe as it decreases.\n",
        "*   If it stops decreasing, then stop training.\n",
        "*   You will be either at the global minima, or very close to it, in which case the prediction will be good enough to use.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AhQyz5gD6DKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$$$\n",
        "### **A note on the learning rate $\\alpha$**\n",
        "\n",
        "---$\\alpha$ (learning rate) should :\n",
        "\n",
        "    1.  not be too large\n",
        "        Otherwise we will overshoot past the optima.\n",
        "\n",
        "    2.  not too small\n",
        "        too small alpha would increase the cost of computation in terms of both time and processing.\n",
        "\n",
        "*Try multiples values of $α$ to see which works best.*\n",
        "\n",
        "*One could for example start on $α = 0.01$, and keep doubling it until an optimal value is identified.*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jw8pWbaDAt3T"
      }
    }
  ]
}